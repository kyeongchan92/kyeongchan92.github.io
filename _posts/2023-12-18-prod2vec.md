
제안된 방법은 이전 구매 내역(이메일로 날라온 영수증)을 이용하여 그 영수증 안의 상품들을 학습하여 추천하는 방법이다. 영수증을 패키지 상품으로, 영수증 안의 상품들 목록을 패키지의 여행 일정으로 대응시켜 하나투어 패키지 상품 추천시스템에 적용 가능할지 확인해볼 필요가 있다. Prod2vec을 발전시킨 Meta-prod2vec이 네이버 상품 추천시스템 중 유사아이템 추천시스템에 참고되었다.
제안된 방법은 상품을 저차원 공간에서의 표현(representation)으로 학습하는 방법을 제안한다. 임베딩 공간 안에서 최근접 이웃을 찾음으로써 추천이 이루어진다.
$$\mathcal{S}$$는 이메일 영수증들의 집합으로써, N명의 유저로부터 얻어진 것이다. 유저의 로그는 $$s=(e_1, e_2, …, e_M)$$으로 구성되며 $$s \in \mathcal{S}$$이다. 각각의 이메일 e는 $$T_m$$개의 상품들 p로 구성되어있음. 
즉, $$e_m = (p_{m_1}, p_{m_2}, …, p_{m_{T_m}})$$이다.

목적 : 각각의 상품 p의 D차원 표현인 $$\mathbf{v}_p$$를 찾는 것. 이 때 당연하지만 유사한 아이템은 근처에 위치해야함.

![0](/assets/images/prod2vec/user_logs.png)*s는 이메일(e)의 시퀀스이며, 이메일은 product로 구성되어있다.*

# 저차원의 상품 임베딩

**prod2vec**

prod2vec 모델은 NLP 분야에서의 용어를 빌리자면 구매 시퀀스를 문장으로, 시퀀스 안의 상품들을 단어로 보고 상품의 벡터 표현을 학습하는 것이다. 본 논문에서는 Skip-gram 방식[24]을 사용하였다. 그리하여 아래의 목적함수를 최대화시킨다. 목적함수란 만약 사각형을 가장 크게 만들고 싶다고 가정할때 사각형의 넓이같은 것을 의미한다. MLE가 대표적인 목적함수이다.

$$\mathcal{L} = \sum_{s \in \mathcal{S}} \sum_{p_i \in s} \sum_{-c \le j \le c, j \ne 0}  \log \mathbb{P}(p_{i+j} | p_i)$$

$$\mathbb{P}(p_{i+j} | p_j)$$는 상품 $$p_i$$가 주어졌을 때 이웃하는 상품 $$p_{i+j}$$를 관측할 확률이며 아래와 같이 소프트맥스 함수로 정의된다.

같은 s 안에 있는 상품들은 임의로 배열된다. $$\mathbb{P}(p_{i+j} | p_i) = \frac{\exp (\mathbf{v}^T_{p_i} \mathbf{v}'\_{p_{i+j}})} {\sum_{p=1}^{P} \exp(\mathbf{v}^T_{p_i} \mathbf{v}'_p)} $$

$$\mathbf{v}_p$$는 인풋, $$\mathbf{v}'_p$$은 아웃풋 벡터 표현을 의미한다. c는 컨텍스트의 길이이다. P는 단어의 수이다.

![0](/assets/images/prod2vec/context_neighbor.png)

**bagged-prod2vec**


